# Training report

## Goals
Trying to learn from the mistakes made in V1, the goal is to build a curriculum as follows:

- The first step is swing control again. Making the agent control its swing by only allowing cabin movement. Increasing difficulty by adding length to the winch. The way I imagine training is by spawning the agent in a random position, with a random target. Giving the agent a reward for reaching the target and giving a reward whenever the swing of the crane stays below a set value. This could be 0,5/MaxStep for the swing reward and 1 for hitting the target(if at max length of cable, otherwise 0,8). This way, training could stop whenever the agent gets a reward of 1,5. The ideal value for rewarding low swing will need to be experimented for. Making this too low will result in the agent not controlling swing, but setting it too high will prevent the agent from moving at all. This could possible be prevented by only giving the swing reward whenever the agent moves. This could however result in the agent only moving barely forwards and backwards so it gets the reward but doesn't actually moves productively. This would also increase the difficulty to set an ideal target to reach before ending training because the agent needs to build up speed.

- The second step will be swing control, combined with winch control. As stated in the report of V1; this would probably be best trained using a radius around the target wherein the winch can be controlled. The way I imagine training is as follows; generate a random position for a target. Then generate a random position for the crane cabin, but keep the spreader height as a random height within a radius around the target height. This radius can then be increased to increase the difficulty. This way the agent learns to use both winch up and down, but also prevents the agent from just keep pulling up or down and relying on the stops that are coded in to help it learn. Rewards can be shaped the same as in the first lesson. Giving a small reward whenever the swing is low, and a large reward whenever the agent gets to the target.

- The third lesson has the same goals as lesson 2, but requires the agent to remain at the target area. As learned in V1 of training, this could take a lot of episodes to master. The first way to approach this is as in V1. Set a time the agent is required to stay at the target location and increase it with every succesful episode. In V1 this was done using a 0.01s increase, stopping at a maximum of 5 seconds. This could be supplemented by adding a dynamic goal distance if the agent does not train well. However, adding a dynamic radius could prevent the agent from learning to stay at the goal since the goal is much larger. If this way of training is implemented, it should be considered to first increase the time to stay to its maximum time. Only after the maximum stay time has been reached, lower the radius to the target to increase accuracy. If both the distance and time are changed simultaniously it might be too difficult for the agent to learn what is rewarded.

If lesson 3 is succesful, the goals of this training session would be completed. After that we could continue training to grab a container.